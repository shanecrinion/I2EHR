% Encoding: UTF-8

@Article{ThenNowFuture,
  author   = {Evans, R. S.},
  title    = {{{E}lectronic {H}ealth {R}ecords: {T}hen, {N}ow, and in the {F}uture}},
  journal  = {Yearb Med Inform},
  year     = {2016},
  volume   = {Suppl 1},
  month    = {May},
  pages    = {48--61},
  keywords = {rank1},
}

@Article{Synthea,
  author   = {Walonoski, J. and Kramer, M. and Nichols, J. and Quina, A. and Moesel, C. and Hall, D. and Duffett, C. and Dube, K. and Gallagher, T. and McLachlan, S.},
  title    = {{{S}ynthea: {A}n approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record}},
  journal  = {J Am Med Inform Assoc},
  year     = {2017},
  month    = {Aug},
  keywords = {rank1},
}

@Article{GEOdatabase,
  author   = {Clough, E. and Barrett, T.},
  title    = {{{T}he {G}ene {E}xpression {O}mnibus {D}atabase}},
  journal  = {Methods Mol. Biol.},
  year     = {2016},
  volume   = {1418},
  pages    = {93--110},
  keywords = {rank1},
}

@Article{CombiningCPG,
  author        = {Villanueva, A. and Hoshida, Y. and Battiston, C. and Tovar, V. and Sia, D. and Alsinet, C. and Cornella, H. and Liberzon, A. and Kobayashi, M. and Kumada, H. and Thung, S. N. and Bruix, J. and Newell, P. and April, C. and Fan, J. B. and Roayaie, S. and Mazzaferro, V. and Schwartz, M. E. and Llovet, J. M.},
  title         = {{{C}ombining clinical, pathology, and gene expression data to predict recurrence of hepatocellular carcinoma}},
  journal       = {Gastroenterology},
  year          = {2011},
  volume        = {140},
  number        = {5},
  month         = {May},
  pages         = {1501--1512},
  __markedentry = {[shane:3]},
  keywords      = {rank1},
}

@Article{Stark2019,
  author   = {Zornitza Stark and Lena Dolman and Teri A. Manolio and Brad Ozenberger and Sue L. Hill and Mark J. Caulfied and Yves Levy and David Glazer and Julia Wilson and Mark Lawler and Tiffany Boughtwood and Jeffrey Braithwaite and Peter Goodhand and Ewan Birney and Kathryn N. North},
  title    = {Integrating Genomics into Healthcare: A Global Responsibility},
  journal  = {The American Journal of Human Genetics},
  year     = {2019},
  volume   = {104},
  number   = {1},
  pages    = {13 - 20},
  issn     = {0002-9297},
  doi      = {https://doi.org/10.1016/j.ajhg.2018.11.014},
  url      = {http://www.sciencedirect.com/science/article/pii/S0002929718304221},
  abstract = {Genomic sequencing is rapidly transitioning into clinical practice, and implementation into healthcare systems has been supported by substantial government investment, totaling over US$4 billion, in at least 14 countries. These national genomic-medicine initiatives are driving transformative change under real-life conditions while simultaneously addressing barriers to implementation and gathering evidence for wider adoption. We review the diversity of approaches and current progress made by national genomic-medicine initiatives in the UK, France, Australia, and US and provide a roadmap for sharing strategies, standards, and data internationally to accelerate implementation.},
  keywords = {rank1},
}

@Article{Rajkomar2018,
  author   = {Rajkomar, Alvin and Oren, Eyal and Chen, Kai and Dai, Andrew M. and Hajaj, Nissan and Hardt, Michaela and Liu, Peter J. and Liu, Xiaobing and Marcus, Jake and Sun, Mimi and Sundberg, Patrik and Yee, Hector and Zhang, Kun and Zhang, Yi and Flores, Gerardo and Duggan, Gavin E. and Irvine, Jamie and Le, Quoc and Litsch, Kurt and Mossin, Alexander and Tansuwan, Justin and Wang, De and Wexler, James and Wilson, Jimbo and Ludwig, Dana and Volchenboum, Samuel L. and Chou, Katherine and Pearson, Michael and Madabushi, Srinivasan and Shah, Nigam H. and Butte, Atul J. and Howell, Michael D. and Cui, Claire and Corrado, Greg S. and Dean, Jeffrey},
  title    = {Scalable and accurate deep learning with electronic health records},
  journal  = {npj Digital Medicine},
  year     = {2018},
  volume   = {1},
  number   = {1},
  month    = may,
  pages    = {18},
  issn     = {2398-6352},
  url      = {https://doi.org/10.1038/s41746-018-0029-1},
  abstract = {Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient’s record. We propose a representation of patients’ entire raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two US academic medical centers with 216,221 adult patients hospitalized for at least 24 h. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting: in-hospital mortality (area under the receiver operator curve [AUROC] across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient’s final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed traditional, clinically-used predictive models in all cases. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios. In a case study of a particular prediction, we demonstrate that neural networks can be used to identify relevant information from the patient’s chart.},
  keywords = {rank1},
  refid    = {Rajkomar2018},
}

@Article{Adkins2017,
  author   = {Adkins, Daniel E.},
  title    = {Machine Learning and Electronic Health Records: A Paradigm Shift},
  journal  = {The American journal of psychiatry},
  year     = {2017},
  volume   = {174},
  number   = {2},
  month    = feb,
  pages    = {93--94},
  issn     = {0002-953X},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/28142275},
  comment  = {28142275[pmid] PMC5807064[pmcid]},
  database = {PubMed},
  keywords = {rank1},
}

@Article{Osmani2018,
  author      = {Venet Osmani and Li Li and Matteo Danieletto and Benjamin Glicksberg and Joel Dudley and Oscar Mayora},
  title       = {Processing of Electronic Health Records using Deep Learning: A review},
  date        = {2018-04-05},
  eprint      = {1804.01758v1},
  eprintclass = {cs.CY},
  eprinttype  = {arXiv},
  abstract    = {Availability of large amount of clinical data is opening up new research avenues in a number of fields. An exciting field in this respect is healthcare, where secondary use of healthcare data is beginning to revolutionize healthcare. Except for availability of Big Data, both medical data from healthcare institutions (such as EMR data) and data generated from health and wellbeing devices (such as personal trackers), a significant contribution to this trend is also being made by recent advances on machine learning, specifically deep learning algorithms.},
  file        = {online:http\://arxiv.org/pdf/1804.01758v1:PDF},
  keywords    = {cs.CY, rank1},
}

@Article{Gebert2018,
  author      = {Theresa Gebert and Shuli Jiang and Jiaxian Sheng},
  title       = {Characterizing Allegheny County Opioid Overdoses with an Interactive Data Explorer and Synthetic Prediction Tool},
  date        = {2018-04-24},
  eprint      = {1804.08830v1},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {The United States has an opioid epidemic, and Pennsylvania's Allegheny County is among the worst. This motivates a deeper exploration of what characterizes the epidemic, such as what are risk factors for people who ultimately overdose and die due to opioids. We show that some interesting trends and factors can be identified from openly available autopsy data, and demonstrate the power of building an interactive data exploration tool for policy makers. However, there is still a pressing need to incorporate further demographic factors. We show this by using synthetic Electronic Medical Record (EMR) data to simulate the predictive power of random forests and neural networks when given additional loosely correlated features. In addition, we give examples of useful feature extraction that enable model enhancement without sacrificing privacy.},
  file        = {online:http\://arxiv.org/pdf/1804.08830v1:PDF},
  keywords    = {stat.AP, q-bio.PE, rank1},
}

@Article{Hong2017,
  author   = {Hong, Na and Prodduturi, Naresh and Wang, Chen and Jiang, Guoqian},
  title    = {Shiny FHIR: An Integrated Framework Leveraging Shiny R and HL7 FHIR to Empower Standards-Based Clinical Data Applications},
  journal  = {Studies in health technology and informatics},
  year     = {2017},
  volume   = {245},
  pages    = {868--872},
  issn     = {0926-9630},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/29295223},
  abstract = {In this study, we describe our efforts in building a clinical statistics and analysis application platform using an emerging clinical data standard, HL7 FHIR, and an open source web application framework, Shiny. We designed two primary workflows that integrate a series of R packages to enable both patient-centered and cohort-based interactive analyses. We leveraged Shiny with R to develop interactive interfaces on FHIR-based data and used ovarian cancer study datasets as a use case to implement a prototype. Specifically, we implemented patient index, patient-centered data report and analysis, and cohort analysis. The evaluation of our study was performed by testing the adaptability of the framework on two public FHIR servers. We identify common research requirements and current outstanding issues, and discuss future enhancement work of the current studies. Overall, our study demonstrated that it is feasible to use Shiny for implementing interactive analysis on FHIR-based standardized clinical data.},
  comment  = {29295223[pmid] PMC5939961[pmcid]},
  database = {PubMed},
  keywords = {rank1},
}

@Article{Kannry2013,
  author   = {Kannry, Joseph L. and Williams, Marc S.},
  title    = {Integration of genomics into the electronic health record: mapping terra incognita},
  journal  = {Genetics in medicine : official journal of the American College of Medical Genetics},
  year     = {2013},
  volume   = {15},
  number   = {10},
  month    = oct,
  pages    = {757--760},
  issn     = {1098-3600},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/24097178},
  abstract = {Successfully realizing the vision of genomic medicine will require management of large amounts of complex data. The electronic health record (EHR) is destined to play a critical role in the translation of genomic information into clinical care. The papers in this special issue explore the challenges associated with the implementation of genomics in the EHR. The proposed solutions are meant to provide guidance for those responsible for moving genomics into the clinic.},
  comment  = {24097178[pmid] PMC4157459[pmcid]},
  database = {PubMed},
  keywords = {rank1},
}

@Article{Seal2018,
  author    = {Seal, Abhik and Wild, David J.},
  title     = {Netpredictor: R and Shiny package to perform drug-target network analysis and prediction of missing links},
  journal   = {BMC bioinformatics},
  year      = {2018},
  volume    = {19},
  number    = {1},
  month     = jul,
  pages     = {265--265},
  issn      = {1471-2105},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/30012095},
  abstract  = {BACKGROUND: Netpredictor is an R package for prediction of missing links in any given unipartite or bipartite network. The package provides utilities to compute missing links in a bipartite and well as unipartite networks using Random Walk with Restart and Network inference algorithm and a combination of both. The package also allows computation of Bipartite network properties, visualization of communities for two different sets of nodes, and calculation of significant interactions between two sets of nodes using permutation based testing. The application can also be used to search for top-K shortest paths between interactome and use enrichment analysis for disease, pathway and ontology. The R standalone package (including detailed introductory vignettes) and associated R Shiny web application is available under the GPL-2 Open Source license and is freely available to download. RESULTS: We compared different algorithms performance in different small datasets and found random walk supersedes rest of the algorithms. The package is developed to perform network based prediction of unipartite and bipartite networks and use the results to understand the functionality of proteins in an interactome using enrichment analysis. CONCLUSION: The rapid application development envrionment like shiny, helps non programmers to develop fast rich visualization apps and we beleieve it would continue to grow in future with further enhancements. We plan to update our algorithms in the package in near future and help scientist to analyse data in a much streamlined fashion.},
  comment   = {30012095[pmid] PMC6047136[pmcid]},
  database  = {PubMed},
  keywords  = {rank1},
  publisher = {BioMed Central},
}

@Article{Son2018,
  author                 = {Son, Jung Hoon and Xie, Gangcai and Yuan, Chi and Ena, Lyudmila and Li, Ziran and Goldstein, Andrew and Huang, Lulin and Wang, Liwei and Shen, Feichen and Liu, Hongfang and Mehl, Karla and Groopman, Emily E. and Marasa, Maddalena and Kiryluk, Krzysztof and Gharavi, Ali G. and Chung, Wendy K. and Hripcsak, George and Friedman, Carol and Weng, Chunhua and Wang, Kai},
  title                  = {Deep Phenotyping on Electronic Health Records Facilitates Genetic Diagnosis by Clinical Exomes.},
  journal                = {American journal of human genetics},
  year                   = {2018},
  language               = {eng},
  volume                 = {103},
  issue                  = {1},
  month                  = {Jul},
  pages                  = {58-73},
  abstract               = {Integration of detailed phenotype information with genetic data is well established to facilitate accurate diagnosis of hereditary disorders. As a rich source of phenotype information, electronic health records (EHRs) promise to empower diagnostic variant interpretation. However, how to accurately and efficiently extract phenotypes from heterogeneous EHR narratives remains a challenge. Here, we present EHR-Phenolyzer, a high-throughput EHR framework for extracting and analyzing phenotypes. EHR-Phenolyzer extracts and normalizes Human Phenotype Ontology (HPO) concepts from EHR narratives and then prioritizes genes with causal variants on the basis of the HPO-coded phenotype manifestations. We assessed EHR-Phenolyzer on 28 pediatric individuals with confirmed diagnoses of monogenic diseases and found that the genes with causal variants were ranked among the top 100 genes selected by EHR-Phenolyzer for 16/28 individuals (p < 2.2 x 10(-16)), supporting the value of phenotype-driven gene prioritization in diagnostic sequence interpretation. To assess the generalizability, we replicated this finding on an independent EHR dataset of ten individuals with a positive diagnosis from a different institution. We then assessed the broader utility by examining two additional EHR datasets, including 31 individuals who were suspected of having a Mendelian disease and underwent different types of genetic testing and 20 individuals with positive diagnoses of specific Mendelian etiologies of chronic kidney disease from exome sequencing. Finally, through several retrospective case studies, we demonstrated how combined analyses of genotype data and deep phenotype data from EHRs can expedite genetic diagnoses. In summary, EHR-Phenolyzer leverages EHR narratives to automate phenotype-driven analysis of clinical exomes or genomes, facilitating the broader implementation of genomic medicine.},
  address                = {United States},
  article-doi            = {10.1016/j.ajhg.2018.05.010},
  article-pii            = {S0002-9297(18)30171-X},
  electronic-issn        = {1537-6605},
  electronic-publication = {20180628},
  grantno                = {R01 HG006465/HG/NHGRI NIH HHS/United States},
  history                = {2018/07/03 06:00 [entrez]},
  keywords               = {biomedical informatics, diagnosis, electronic health records, exome, genome, knowledge engineering, natural language processing, next-generation sequencing, phenotyping, precision medicine, rank1},
  linking-issn           = {0002-9297},
  location-id            = {10.1016/j.ajhg.2018.05.010 [doi]},
  nlm-unique-id          = {0370475},
  owner                  = {NLM},
  publication-status     = {ppublish},
  revised                = {20190107},
  source                 = {Am J Hum Genet. 2018 Jul 5;103(1):58-73. doi: 10.1016/j.ajhg.2018.05.010. Epub 2018 Jun 28.},
  status                 = {In-Data-Review},
  termowner              = {NOTNLM},
  title-abbreviation     = {Am J Hum Genet},
}

@Article{Chiu2017,
  author                 = {Chiu, Po-Hsiang and Hripcsak, George},
  title                  = {{E}{H}{R}-based phenotyping: Bulk learning and evaluation.},
  journal                = {Journal of biomedical informatics},
  year                   = {2017},
  language               = {eng},
  volume                 = {70},
  month                  = {Jun},
  pages                  = {35-51},
  abstract               = {In data-driven phenotyping, a core computational task is to identify medical concepts and their variations from sources of electronic health records (EHR) to stratify phenotypic cohorts. A conventional analytic framework for phenotyping largely uses a manual knowledge engineering approach or a supervised learning approach where clinical cases are represented by variables encompassing diagnoses, medicinal treatments and laboratory tests, among others. In such a framework, tasks associated with feature engineering and data annotation remain a tedious and expensive exercise, resulting in poor scalability. In addition, certain clinical conditions, such as those that are rare and acute in nature, may never accumulate sufficient data over time, which poses a challenge to establishing accurate and informative statistical models. In this paper, we use infectious diseases as the domain of study to demonstrate a hierarchical learning method based on ensemble learning that attempts to address these issues through feature abstraction. We use a sparse annotation set to train and evaluate many phenotypes at once, which we call bulk learning. In this batch-phenotyping framework, disease cohort definitions can be learned from within the abstract feature space established by using multiple diseases as a substrate and diagnostic codes as surrogates. In particular, using surrogate labels for model training renders possible its subsequent evaluation using only a sparse annotated sample. Moreover, statistical models can be trained and evaluated, using the same sparse annotation, from within the abstract feature space of low dimensionality that encapsulates the shared clinical traits of these target diseases, collectively referred to as the bulk learning set.},
  address                = {United States},
  article-doi            = {10.1016/j.jbi.2017.04.009},
  article-pii            = {S1532-0464(17)30079-5},
  completed              = {20171204},
  electronic-issn        = {1532-0480},
  electronic-publication = {20170412},
  grantno                = {R01 LM006910/LM/NLM NIH HHS/United States},
  history                = {2017/04/16 06:00 [entrez]},
  keywords               = {Data Curation, *Electronic Health Records, Humans, Models, Statistical, Phenotype, *Supervised Machine Learning, *Disease modeling, *EHR phenotyping, *Ensemble learning, *Feature learning, *Knowledge representation, *Stacked generalization, rank1},
  linking-issn           = {1532-0464},
  location-id            = {10.1016/j.jbi.2017.04.009 [doi]},
  manuscript-id          = {NIHMS962422},
  nlm-unique-id          = {100970413},
  owner                  = {NLM},
  publication-status     = {ppublish},
  revised                = {20181202},
  source                 = {J Biomed Inform. 2017 Jun;70:35-51. doi: 10.1016/j.jbi.2017.04.009. Epub 2017 Apr 12.},
  status                 = {MEDLINE},
  subset                 = {IM},
  termowner              = {NOTNLM},
  title-abbreviation     = {J Biomed Inform},
}

@Article{Brown2019,
  author   = {Brown, Sherry-Ann N. and Jouni, Hayan and Kullo, Iftikhar J.},
  title    = {Electronic health record access by patients as an indicator of information seeking and sharing for cardiovascular health promotion in social networks: Secondary analysis of a randomized clinical trial},
  journal  = {Preventive Medicine Reports},
  year     = {2019},
  volume   = {13},
  month    = mar,
  pages    = {306--313},
  issn     = {2211-3355},
  url      = {http://www.sciencedirect.com/science/article/pii/S2211335518302845},
  abstract = {We investigated electronic health record (EHR) access as an indicator of cardiovascular health promotion by patients in their social networks, by identifying individuals who viewed their coronary heart disease (CHD) risk information in the EHR and shared this information in their social networks among various spheres of influence. In a secondary analysis of the Myocardial Infarction Genes trial, Olmsted County MN residents (2013-2015; n = 203; whites, ages 45-65 years) at intermediate CHD risk were randomized to receive their conventional risk score (CRS; based on traditional risk factors) alone or also their genetic risk score (GRS; based on 28 genomic variants). We assessed self-reported and objectively quantified EHR access via a patient portal at three and six months after risk disclosure, and determined whether this differed by GRS disclosure. Data were analyzed using logistic regression and adjusted for sociodemographic characteristics, family history, and baseline CRS/GRS. Self-reported EHR access to view CHD risk information was associated with a high frequency of objectively quantified EHR access (71(10) versus 37(5) logins; P = 0.0025) and a high likelihood of encouraging others to be screened for their CHD risk (OR 2.936, CI 1.443-5.973, P = 0.0030), compared to the absence of self-reported EHR access to view CHD risk information. We thereby used EHR access trends to identify individuals who may function as disseminators of CHD risk information in social networks, compared to individuals on the periphery of their social networks who did not exhibit this behavior. Partnering with such individuals could amplify CHD health promotion. Clinical Trial Registration: Myocardial Infarction Genes (MI-GENES) Study, NCT01936675, https://clinicaltrials.gov/ct2/show/NCT01936675.},
  keywords = {Genetics, Risk factors, Risk assessment, Behavior modification, Electronic health records, Personal health records, Patient portals, Patient engagement, Social network, rank1},
}

@InCollection{Williams2019,
  author    = {Williams, Marc S.},
  title     = {The Genomic Health Record: Current Status and Vision for the Future},
  booktitle = {Emery and Rimoin's Principles and Practice of Medical Genetics and Genomics (Seventh Edition)},
  year      = {2019},
  editor    = {Pyeritz, Reed E. and Korf, Bruce R. and Grody, Wayne W.},
  publisher = {Academic Press},
  pages     = {315--325},
  url       = {http://www.sciencedirect.com/science/article/pii/B9780128125366000122},
  abstract  = {Genetics and genomics are emerging in clinical practice. The volume and complexity of the information exceed the capacity of even the best providers. This results in medical decisions that are not fully informed by this important information. Clinical information systems such as electronic health records have the potential to aid the provider as they synthesize data about the patient and translate that into medical decisions. Currently, electronic health record systems have significant deficiencies that limit their use for genetic and genomic information. The chapter examines the current state of the electronic health record, catalogs progress in achieving a genomic-enabled electronic health record system, and articulates a vision for the electronic health record in the era of genomic medicine.},
  issn      = {978-0-12-812536-6},
  keywords  = {Applications, Clinical decision support, Electronic health record, Genetics, Genomics, Infobutton, “Just-in-time” education, Knowledge representation, rank1},
  month     = jan,
}

@Article{Zhang2019,
  author   = {Zhang, Xingmin Aaron and Yates, Amy and Vasilevsky, Nicole and Gourdine, J. P. and Carmody, Leigh C. and Danis, Daniel and Joachimiak, Marcin P. and Ravanmehr, Vida and Pfaff, Emily R. and Champion, James and Robasky, Kimberly and Xu, Hao and Fecho, Karamarie and Walton, Nephi A. and Zhu, Richard and Ramsdill, Justin and Mungall, Chris and Kohler, Sebastian and Haendel, Melissa A. and McDonald, Clem and Vreeman, Daniel J. and Peden, David B. and Chute, Christopher G. and Robinson, Peter N.},
  title    = {Semantic Integration of Clinical Laboratory Tests from Electronic Health Records for Deep Phenotyping and Biomarker Discovery},
  journal  = {bioRxiv},
  year     = {2019},
  month    = jan,
  pages    = {519231},
  url      = {http://biorxiv.org/content/early/2019/01/13/519231.abstract},
  abstract = {Electronic Health Record (EHR) systems typically define laboratory test results using the Laboratory Observation Identifier Names and Codes (LOINC) and can transmit them using Fast Healthcare Interoperability Resource (FHIR) standards. LOINC has not yet been semantically integrated with computational resources for phenotype analysis. Here, we provide a method for mapping LOINC-encoded laboratory test results transmitted in FHIR standards to the Human Phenotype Ontology (HPO) terms. We annotated the medical implications of 2421 commonly used laboratory tests with HPO terms. Using these annotations, a software assesses laboratory test results and converts each into an HPO term. We validated our approach with EHR data from 15,681 patients with respiratory complaints and identified known biomarkers for asthma. Finally, we provide a freely available SMART on FHIR application that can be used within EHR systems. Our approach allows reusing readily available laboratory tests in EHR for deep phenotyping and using the hierarchical structure of HPO for association studies with medical outcomes and genomics.},
  keywords = {rank1},
}

@Article{Steele2018,
  author                 = {Steele, Andrew J. and Denaxas, Spiros C. and Shah, Anoop D. and Hemingway, Harry and Luscombe, Nicholas M.},
  title                  = {Machine learning models in electronic health records can outperform conventional survival models for predicting patient mortality in coronary artery disease.},
  journal                = {PloS one},
  year                   = {2018},
  language               = {eng},
  volume                 = {13},
  issue                  = {8},
  pages                  = {e0202344},
  abstract               = {Prognostic modelling is important in clinical practice and epidemiology for patient management and research. Electronic health records (EHR) provide large quantities of data for such models, but conventional epidemiological approaches require significant researcher time to implement. Expert selection of variables, fine-tuning of variable transformations and interactions, and imputing missing values are time-consuming and could bias subsequent analysis, particularly given that missingness in EHR is both high, and may carry meaning. Using a cohort of 80,000 patients from the CALIBER programme, we compared traditional modelling and machine-learning approaches in EHR. First, we used Cox models and random survival forests with and without imputation on 27 expert-selected, preprocessed variables to predict all-cause mortality. We then used Cox models, random forests and elastic net regression on an extended dataset with 586 variables to build prognostic models and identify novel prognostic factors without prior expert input. We observed that data-driven models used on an extended dataset can outperform conventional models for prognosis, without data preprocessing or imputing missing values. An elastic net Cox regression based with 586 unimputed variables with continuous values discretised achieved a C-index of 0.801 (bootstrapped 95% CI 0.799 to 0.802), compared to 0.793 (0.791 to 0.794) for a traditional Cox model comprising 27 expert-selected variables with imputation for missing values. We also found that data-driven models allow identification of novel prognostic variables; that the absence of values for particular variables carries meaning, and can have significant implications for prognosis; and that variables often have a nonlinear association with mortality, which discretised Cox models and random forests can elucidate. This demonstrates that machine-learning approaches applied to raw EHR data can be used to build models for use in research and clinical practice, and identify novel predictive variables and their effects to inform future research.},
  address                = {United States},
  article-doi            = {10.1371/journal.pone.0202344},
  article-pii            = {PONE-D-18-03097},
  completed              = {20190213},
  electronic-issn        = {1932-6203},
  electronic-publication = {20180831},
  grantno                = {Chief Scientist Office (UK)/International},
  history                = {2019/02/14 06:00 [medline]},
  keywords               = {Adult, Aged, 80 and over, Cohort Studies, Coronary Artery Disease/*diagnosis/*mortality, Data Interpretation, Statistical, Diagnosis, Computer-Assisted/*methods, *Electronic Health Records, Female, Humans, *Machine Learning, Male, Middle Aged, Models, Biological, Prognosis, *Survival Analysis, rank1},
  linking-issn           = {1932-6203},
  location-id            = {10.1371/journal.pone.0202344 [doi]},
  nlm-unique-id          = {101285081},
  owner                  = {NLM},
  publication-status     = {epublish},
  revised                = {20190215},
  source                 = {PLoS One. 2018 Aug 31;13(8):e0202344. doi: 10.1371/journal.pone.0202344. eCollection 2018.},
  status                 = {MEDLINE},
  subset                 = {IM},
  title-abbreviation     = {PLoS One},
}

@Article{Hall2016,
  author   = {Hall et al},
  title    = {Merging Electronic Health Record Data and Genomics for Cardiovascular Research: A Science Advisory From the American Heart Association},
  journal  = {Circulation. Cardiovascular genetics},
  year     = {2016},
  volume   = {9},
  number   = {2},
  month    = apr,
  pages    = {193--202},
  issn     = {1942-325X},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/26976545},
  abstract = {The process of scientific discovery is rapidly evolving. The funding climate has influenced a favorable shift in scientific discovery toward the use of existing resources such as the electronic health record. The electronic health record enables long-term outlooks on human health and disease, in conjunction with multidimensional phenotypes that include laboratory data, images, vital signs, and other clinical information. Initial work has confirmed the utility of the electronic health record for understanding mechanisms and patterns of variability in disease susceptibility, disease evolution, and drug responses. The addition of biobanks and genomic data to the information contained in the electronic health record has been demonstrated. The purpose of this statement is to discuss the current challenges in and the potential for merging electronic health record data and genomics for cardiovascular research.},
  comment  = {26976545[pmid] PMC5646218[pmcid]},
  database = {PubMed},
  edition  = {2016/03/14},
  keywords = {rank1},
}

@Article{Hurle2013,
  author                 = {Hurle, M. R. and Yang, L. and Xie, Q. and Rajpal, D. K. and Sanseau, P. and Agarwal, P.},
  title                  = {Computational drug repositioning: from data to therapeutics.},
  journal                = {Clinical pharmacology and therapeutics},
  year                   = {2013},
  language               = {eng},
  volume                 = {93},
  issue                  = {4},
  month                  = {Apr},
  pages                  = {335-41},
  __markedentry          = {[shane:3]},
  abstract               = {Traditionally, most drugs have been discovered using phenotypic or target-based screens. Subsequently, their indications are often expanded on the basis of clinical observations, providing additional benefit to patients. This review highlights computational techniques for systematic analysis of transcriptomics (Connectivity Map, CMap), side effects, and genetics (genome-wide association study, GWAS) data to generate new hypotheses for additional indications. We also discuss data domains such as electronic health records (EHRs) and phenotypic screening that we consider promising for novel computational repositioning methods.},
  address                = {United States},
  article-doi            = {10.1038/clpt.2013.1},
  article-pii            = {clpt20131},
  completed              = {20130517},
  electronic-issn        = {1532-6535},
  electronic-publication = {20130115},
  history                = {2013/05/18 06:00 [medline]},
  keywords               = {Computational Biology/*methods, Databases, Genetic, Drug Discovery/*methods, *Drug Repositioning, Electronic Health Records, Humans, Transcriptome/*drug effects, rank1},
  linking-issn           = {0009-9236},
  location-id            = {10.1038/clpt.2013.1 [doi]},
  nlm-unique-id          = {0372741},
  owner                  = {NLM},
  publication-status     = {ppublish},
  revised                = {20130320},
  source                 = {Clin Pharmacol Ther. 2013 Apr;93(4):335-41. doi: 10.1038/clpt.2013.1. Epub 2013 Jan 15.},
  status                 = {MEDLINE},
  subset                 = {IM},
  title-abbreviation     = {Clin Pharmacol Ther},
}

@Article{Robinson2012,
  author             = {Robinson, Peter N.},
  title              = {Deep phenotyping for precision medicine.},
  journal            = {Human mutation},
  year               = {2012},
  language           = {eng},
  volume             = {33},
  issue              = {5},
  month              = {May},
  pages              = {777-80},
  __markedentry      = {[shane:3]},
  abstract           = {In medical contexts, the word "phenotype" is used to refer to some deviation from normal morphology, physiology, or behavior. The analysis of phenotype plays a key role in clinical practice and medical research, and yet phenotypic descriptions in clinical notes and medical publications are often imprecise. Deep phenotyping can be defined as the precise and comprehensive analysis of phenotypic abnormalities in which the individual components of the phenotype are observed and described. The emerging field of precision medicine aims to provide the best available care for each patient based on stratification into disease subclasses with a common biological basis of disease. The comprehensive discovery of such subclasses, as well as the translation of this knowledge into clinical care, will depend critically upon computational resources to capture, store, and exchange phenotypic data, and upon sophisticated algorithms to integrate it with genomic variation, omics profiles, and other clinical information. This special issue of Human Mutation offers a number of articles describing computational solutions for current challenges in deep phenotyping, including semantic and technical standards for phenotype and disease data, digital imaging for facial phenotype analysis, model organism phenotypes, and databases for correlating phenotypes with genomic variation.},
  address            = {United States},
  article-doi        = {10.1002/humu.22080},
  completed          = {20120730},
  electronic-issn    = {1098-1004},
  history            = {2012/07/31 06:00 [medline]},
  keywords           = {Animals, Databases as Topic, Genetic Predisposition to Disease, Humans, Medical Informatics, *Phenotype, *Precision Medicine, Risk Factors, rank1},
  linking-issn       = {1059-7794},
  location-id        = {10.1002/humu.22080 [doi]},
  nlm-unique-id      = {9215429},
  owner              = {NLM},
  publication-status = {ppublish},
  revised            = {20151119},
  source             = {Hum Mutat. 2012 May;33(5):777-80. doi: 10.1002/humu.22080.},
  status             = {MEDLINE},
  subset             = {IM},
  title-abbreviation = {Hum Mutat},
}

@Article{Floyd2016,
  author        = {Floyd, James S. and Psaty, Bruce M.},
  title         = {The Application of Genomics in Diabetes: Barriers to Discovery and Implementation},
  journal       = {Diabetes Care},
  year          = {2016},
  volume        = {39},
  number        = {11},
  pages         = {1858--1869},
  issn          = {0149-5992},
  doi           = {10.2337/dc16-0738},
  eprint        = {http://care.diabetesjournals.org/content/39/11/1858.full.pdf},
  url           = {http://care.diabetesjournals.org/content/39/11/1858},
  __markedentry = {[shane:5]},
  abstract      = {The emerging availability of genomic and electronic health data in large populations is a powerful tool for research that has drawn interest in bringing precision medicine to diabetes. In this article, we discuss the potential application of genomics to the prediction, prevention, and treatment of diabetes, and we use examples from other areas of medicine to illustrate some of the challenges involved in conducting genomics research in human populations and implementing findings in practice. At this time, a major barrier to the application of genomics in diabetes care is the lack of actionable genomic findings. Whether genomic information should be used in clinical practice requires a framework for evaluating the validity and clinical utility of this approach, an improved integration of genomic data into electronic health records, and the clinical decision support and educational resources for clinicians to use these data. Efforts to identify optimal approaches in all of these domains are in progress and may help to bring diabetes into the era of genomic medicine.},
  keywords      = {rank1},
  publisher     = {American Diabetes Association},
}

@Article{Khennou2018,
  author   = {Khennou, Fadoua and Khamlichi, Youness Idrissi and Chaoui, Nour El Houda},
  title    = {Improving the Use of Big Data Analytics within Electronic Health Records: A Case Study based OpenEHR},
  journal  = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
  year     = {2018},
  volume   = {127},
  month    = jan,
  pages    = {60--68},
  issn     = {1877-0509},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050918301091},
  abstract = {Recently there has been an increasing adoption of electronic health records (EHRs) in different countries. Thanks to these systems, multiple health bodies can now store, manage and process their data effectively. However, the existence of such powerful and meticulous entities raise new challenges and issues for health practitioners. In fact, while the main objective of EHRs is to gain actionable big data insights from the health workflow, very few physicians exploit widely analytic tools, this is mainly due to the fact of having to deal with multiple systems and steps, which completely discourage them from engaging more and more. In this paper, we shed light and explore precisely the proper adaptation of analytical tools to EHRs in order to upgrade their use by health practitioners. For that, we present a case study of the implementation process of an EHR based OpenEHR and investigate health analytics adoption in each step of the methodology.},
  keywords = {Electronic Health Records, EHRs, Analytic tools, Big Data, Health Practitioners, rank1},
}

@Article{Clough2016,
  author   = {Clough, Emily and Barrett, Tanya},
  title    = {The {G}ene {E}xpression {O}mnibus Database},
  journal  = {Methods in molecular biology (Clifton, N.J.)},
  year     = {2016},
  volume   = {1418},
  pages    = {93--110},
  issn     = {1064-3745},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/27008011},
  abstract = {The Gene Expression Omnibus (GEO) database is an international public repository that archives and freely distributes high-throughput gene expression and other functional genomics data sets. Created in 2000 as a worldwide resource for gene expression studies, GEO has evolved with rapidly changing technologies and now accepts high-throughput data for many other data applications, including those that examine genome methylation, chromatin structure, and genome-protein interactions. GEO supports community-derived reporting standards that specify provision of several critical study elements including raw data, processed data, and descriptive metadata. The database not only provides access to data for tens of thousands of studies, but also offers various Web-based tools and strategies that enable users to locate data relevant to their specific interests, as well as to visualize and analyze the data. This chapter includes detailed descriptions of methods to query and download GEO data and use the analysis and visualization tools. The GEO homepage is at http://www.ncbi.nlm.nih.gov/geo/.},
  comment  = {27008011[pmid] PMC4944384[pmcid]},
  database = {PubMed},
  keywords = {rank1},
}

@Article{Kulynych2017,
  author        = {Kulynych, Jennifer and Greely, Henry T.},
  title         = {Clinical genomics, big data, and electronic medical records: reconciling patient rights with research when privacy and science collide},
  journal       = {Journal of law and the biosciences},
  year          = {2017},
  volume        = {4},
  number        = {1},
  month         = jan,
  pages         = {94--132},
  issn          = {2053-9711},
  url           = {https://www.ncbi.nlm.nih.gov/pubmed/28852559},
  __markedentry = {[shane:5]},
  abstract      = {Widespread use of medical records for research, without consent, attracts little scrutiny compared to biospecimen research, where concerns about genomic privacy prompted recent federal proposals to mandate consent. This paper explores an important consequence of the proliferation of electronic health records (EHRs) in this permissive atmosphere: with the advent of clinical gene sequencing, EHR-based secondary research poses genetic privacy risks akin to those of biospecimen research, yet regulators still permit researchers to call gene sequence data 'de-identified', removing such data from the protection of the federal Privacy Rule and federal human subjects regulations. Medical centers and other providers seeking to offer genomic 'personalized medicine' now confront the problem of governing the secondary use of clinical genomic data as privacy risks escalate. We argue that regulators should no longer permit HIPAA-covered entities to treat dense genomic data as de-identified health information. Even with this step, the Privacy Rule would still permit disclosure of clinical genomic data for research, without consent, under a data use agreement, so we also urge that providers give patients specific notice before disclosing clinical genomic data for research, permitting (where possible) some degree of choice and control. To aid providers who offer clinical gene sequencing, we suggest both general approaches and specific actions to reconcile patients' rights and interests with genomic research.},
  comment       = {28852559[pmid] PMC5570692[pmcid]},
  database      = {PubMed},
  keywords      = {rank1},
  publisher     = {Oxford University Press},
}

@Article{Denny2012,
  author        = {Denny, Joshua C.},
  title         = {Mining Electronic Health Records in the Genomics Era},
  journal       = {PLOS Computational Biology},
  year          = {2012},
  volume        = {8},
  number        = {12},
  month         = dec,
  pages         = {e1002823},
  doi           = {10.1371/journal.pcbi.1002823},
  url           = {https://doi.org/10.1371/journal.pcbi.1002823},
  __markedentry = {[shane:5]},
  abstract      = {Abstract: The combination of improved genomic analysis methods, decreasing genotyping costs, and increasing computing resources has led to an explosion of clinical genomic knowledge in the last decade. Similarly, healthcare systems are increasingly adopting robust electronic health record (EHR) systems that not only can improve health care, but also contain a vast repository of disease and treatment data that could be mined for genomic research. Indeed, institutions are creating EHR-linked DNA biobanks to enable genomic and pharmacogenomic research, using EHR data for phenotypic information. However, EHRs are designed primarily for clinical care, not research, so reuse of clinical EHR data for research purposes can be challenging. Difficulties in use of EHR data include: data availability, missing data, incorrect data, and vast quantities of unstructured narrative text data. Structured information includes billing codes, most laboratory reports, and other variables such as physiologic measurements and demographic information. Significant information, however, remains locked within EHR narrative text documents, including clinical notes and certain categories of test results, such as pathology and radiology reports. For relatively rare observations, combinations of simple free-text searches and billing codes may prove adequate when followed by manual chart review. However, to extract the large cohorts necessary for genome-wide association studies, natural language processing methods to process narrative text data may be needed. Combinations of structured and unstructured textual data can be mined to generate high-validity collections of cases and controls for a given condition. Once high-quality cases and controls are identified, EHR-derived cases can be used for genomic discovery and validation. Since EHR data includes a broad sampling of clinically-relevant phenotypic information, it may enable multiple genomic investigations upon a single set of genotyped individuals. This chapter reviews several examples of phenotype extraction and their application to genetic research, demonstrating a viable future for genomic discovery using EHR-linked data.},
  keywords      = {rank1},
  publisher     = {Public Library of Science},
}

@Article{Moreno-Iribas2017,
  author    = {Moreno-Iribas, Conchi and Sayon-Orea, Carmen and Delfrade, Josu and Ardanaz, Eva and Gorricho, Javier and Burgui, Rosana and Nuin, Marian and Guevara, Marcela},
  title     = {Validity of type 2 diabetes diagnosis in a population-based electronic health record database},
  journal   = {BMC medical informatics and decision making},
  year      = {2017},
  volume    = {17},
  number    = {1},
  month     = apr,
  pages     = {34--34},
  issn      = {1472-6947},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/28390396},
  abstract  = {BACKGROUND: The increasing burden of type 2 diabetes mellitus makes the continuous surveillance of its prevalence and incidence advisable. Electronic health records (EHRs) have great potential for research and surveillance purposes; however the quality of their data must first be evaluated for fitness for use. The aim of this study was to assess the validity of type 2 diabetes diagnosis in a primary care EHR database covering more than half a million inhabitants, 97% of the population in Navarra, Spain. METHODS: In the Navarra EPIC-InterAct study, the validity of the T90 code from the International Classification of Primary Care, Second Edition was studied in a primary care EHR database to identify incident cases of type 2 diabetes, using a multi-source approach as the gold standard. The sensitivity, specificity, positive predictive value, negative predictive value and the kappa index were calculated. Additionally, type 2 diabetes prevalence from the EHR database was compared with estimations from a health survey. RESULTS: The sensitivity, specificity, positive predictive value and negative predictive value of incident type 2 diabetes recorded in the EHRs were 98.2, 99.3, 92.2 and 99.8%, respectively, and the kappa index was 0.946. Overall prevalence of type 2 diabetes diagnosed in the EHRs among adults (35-84 years of age) was 7.2% (95% confidence interval [CI] 7.2-7.3) in men and 5.9% (95% CI 5.8-5.9) in women, which was similar to the prevalence estimated from the health survey: 8.5% (95% CI 7.1-9.8) and 5.5% (95% CI 4.4-6.6) in men and women, respectively. CONCLUSIONS: The high sensitivity and specificity of type 2 diabetes diagnosis found in the primary care EHRs make this database a good source for population-based surveillance of incident and prevalent type 2 diabetes, as well as for monitoring quality of care and health outcomes in diabetic patients.},
  comment   = {28390396[pmid] PMC5385005[pmcid]},
  database  = {PubMed},
  keywords  = {rank1},
  publisher = {BioMed Central},
}

@Article{Shen2018,
  author    = {Shen, Juan and Zhu, Bin},
  title     = {Integrated analysis of the gene expression profile and DNA methylation profile of obese patients with type 2 diabetes},
  journal   = {Molecular medicine reports},
  year      = {2018},
  volume    = {17},
  number    = {6},
  month     = jun,
  pages     = {7636--7644},
  issn      = {1791-2997},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/29620215},
  abstract  = {In order to better understand the etiology of obese type 2 diabetes (T2D) at the molecular level, the present study investigated the gene expression and DNA methylation profiles associated with T2D via systemic analysis. Gene expression (GSE64998) and DNA methylation profiles (GSE65057) from liver tissues of healthy controls and obese patients with T2D were downloaded from the Gene Expression Omnibus database. Differentially‑expressed genes (DEGs) and differentially‑methylated genes (DMGs) were identified using the Limma package, and their overlapping genes were additionally determined. Enrichment analysis was performed using the BioCloud platform on the DEGs and the overlapping genes. Using Cytoscape software, protein‑protein interaction (PPI), transcription factor target networks and microRNA (miRNA) target networks were then constructed in order to determine associated hub genes. In addition, a further GSE15653 dataset was utilized in order to validate the DEGs identified in the GSE64998 dataset analyses. A total of 251 DEGs, including 124 upregulated and 127 downregulated genes, were detected, and a total of 9,698 genes were demonstrated to be differentially methylated in obese patients with T2D compared with non‑obese healthy controls. A total of 103 overlapping genes between the two datasets were revealed, including 47 upregulated genes and 56 downregulated genes. The identified overlapping genes were revealed to be strongly associated with fatty acid and glucose metabolic pathways, in addition to oxidation/reduction. The overlapping genes cyclin D1 (CCND1), PPARG coactivator α (PPARGC1A), fatty acid synthase (FASN), glucokinase (GCK), steraroyl‑coA desaturase (SCD) and tyrosine aminotransferase (TAT) had higher degrees in the PPI, transcription target networks and miRNA target networks. In addition, among the 251 DEGs, a total of 35 DEGs were validated to be being shared genes between the datasets, which included a number of key genes in the PPI network, including CCND1, FASN and TAT. Abnormal gene expression and DNA methylation patterns that were implicated in fatty acid and glucose metabolic pathways and oxidation/reduction reactions were detected in obese patients with T2D. Furthermore, the CCND1, PPARGC1A, FANS, GCK, SCD and TAT genes may serve a role in the development of obesity‑associated T2D.},
  comment   = {29620215[pmid] PMC5983955[pmcid]},
  database  = {PubMed},
  edition   = {2018/03/28},
  keywords  = {rank1},
  publisher = {D.A. Spandidos},
}

@Article{Robbins2018,
  author    = {Robbins, Tim and Lim Choi Keung, Sarah N. and Sankar, Sailesh and Randeva, Harpal and Arvanitis, Theodoros N.},
  title     = {Diabetes and the direct secondary use of electronic health records: Using routinely collected and stored data to drive research and understanding},
  journal   = {Digital health},
  year      = {2018},
  volume    = {4},
  month     = oct,
  pages     = {2055207618804650--2055207618804650},
  issn      = {2055-2076},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/30305917},
  abstract  = {INTRODUCTION: Electronic health records provide an unparalleled opportunity for the use of patient data that is routinely collected and stored, in order to drive research and develop an epidemiological understanding of disease. Diabetes, in particular, stands to benefit, being a data-rich, chronic-disease state. This article aims to provide an understanding of the extent to which the healthcare sector is using routinely collected and stored data to inform research and epidemiological understanding of diabetes mellitus. METHODS: Narrative literature review of articles, published in both the medical- and engineering-based informatics literature. RESULTS: There has been a significant increase in the number of papers published, which utilise electronic health records as a direct data source for diabetes research. These articles consider a diverse range of research questions. Internationally, the secondary use of electronic health records, as a research tool, is most prominent in the USA. The barriers most commonly described in research studies include missing values and misclassification, alongside challenges of establishing the generalisability of results. DISCUSSION: Electronic health record research is an important and expanding area of healthcare research. Much of the research output remains in the form of conference abstracts and proceedings, rather than journal articles. There is enormous opportunity within the United Kingdom to develop these research methodologies, due to national patient identifiers. Such a healthcare context may enable UK researchers to overcome many of the barriers encountered elsewhere and thus to truly unlock the potential of electronic health records.},
  comment   = {30305917[pmid] PMC6176528[pmcid]},
  database  = {PubMed},
  keywords  = {rank1},
  publisher = {SAGE Publications},
}

@Article{Kashyap2015,
  author    = {Kashyap, Meghana V. and Nolan, Michael and Sprouse, Marc and Chakraborty, Ranajit and Cross, Deanna and Roby, Rhonda and Vishwanatha, Jamboor K.},
  title     = {Role of genomics in eliminating health disparities},
  journal   = {Journal of carcinogenesis},
  year      = {2015},
  volume    = {14},
  month     = sep,
  pages     = {6--6},
  issn      = {1477-3163},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/26435701},
  abstract  = {The Texas Center for Health Disparities, a National Institute on Minority Health and Health Disparities Center of Excellence, presents an annual conference to discuss prevention, awareness education, and ongoing research about health disparities both in Texas and among the national population. The 2014 Annual Texas Conference on Health Disparities brought together experts in research, patient care, and community outreach on the "Role of Genomics in Eliminating Health Disparities." Rapid advances in genomics and pharmacogenomics are leading the field of medicine to use genetics and genetic risk to build personalized or individualized medicine strategies. We are at a critical juncture of ensuring such rapid advances benefit diverse populations. Relatively few forums have been organized around the theme of the role of genomics in eliminating health disparities. The conference consisted of three sessions addressing "Gene-Environment Interactions and Health Disparities," "Personalized Medicine and Elimination of Health Disparities," and "Ethics and Public Policy in the Genomic Era." This article summarizes the basic science, clinical correlates, and public health data presented by the speakers.},
  comment   = {26435701[pmid] PMC4590179[pmcid]},
  database  = {PubMed},
  publisher = {Medknow Publications \& Media Pvt Ltd},
}

@Article{Schwartz2018,
  author    = {Schwartz, Marci L. B. and McCormick, Cara Zayac and Lazzeri, Amanda L. and Lindbuchler, D'Andra M. and Hallquist, Miranda L. G. and Manickam, Kandamurugu and Buchanan, Adam H. and Rahm, Alanna Kulchak and Giovanni, Monica A. and Frisbie, Lauren and Flansburg, Carroll N. and Davis, F. Daniel and Sturm, Amy C. and Nicastro, Christine and Lebo, Matthew S. and Mason-Suares, Heather and Mahanta, Lisa Marie and Carey, David J. and Williams, Janet L. and Williams, Marc S. and Ledbetter, David H. and Faucett, W. Andrew and Murray, Michael F.},
  title     = {A Model for Genome-First Care: Returning Secondary Genomic Findings to Participants and Their Healthcare Providers in a Large Research Cohort},
  journal   = {American journal of human genetics},
  year      = {2018},
  volume    = {103},
  number    = {3},
  month     = sep,
  pages     = {328--337},
  issn      = {0002-9297},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/30100086},
  abstract  = {There is growing interest in communicating clinically relevant DNA sequence findings to research participants who join projects with a primary research goal other than the clinical return of such results. Since Geisinger's MyCode Community Health Initiative (MyCode) was launched in 2007, more than 200,000 participants have been broadly consented for discovery research. In 2013 the MyCode consent was amended to include a secondary analysis of research genomic sequences that allows for delivery of clinical results. Since May 2015, pathogenic and likely pathogenic variants from a set list of genes associated with monogenic conditions have prompted "genome-first" clinical encounters. The encounters are described as genome-first because they are identified independent of any clinical parameters. This article (1) details our process for generating clinical results from research data, delivering results to participants and providers, facilitating condition-specific clinical evaluations, and promoting cascade testing of relatives, and (2) summarizes early results and participant uptake. We report on 542 participants who had results uploaded to the electronic health record as of February 1, 2018 and 291 unique clinical providers notified with one or more participant results. Of these 542 participants, 515 (95.0%) were reached to disclose their results and 27 (5.0%) were lost to follow-up. We describe an exportable model for delivery of clinical care through secondary use of research data. In addition, subject and provider participation data from the initial phase of these efforts can inform other institutions planning similar programs.},
  comment   = {30100086[pmid] PMC6128218[pmcid]},
  database  = {PubMed},
  edition   = {2018/08/09},
  publisher = {Elsevier},
}

@Article{Johnson2018,
  author   = {Johnson, Kipp W. and Dudley, Joel T. and Glicksberg, Benjamin S.},
  title    = {The next generation of precision medicine: observational studies, electronic health records, biobanks and continuous monitoring},
  journal  = {hmg},
  year     = {2018},
  volume   = {27},
  number   = {R1},
  month    = apr,
  pages    = {R56--R62},
  issn     = {0964-6906},
  url      = {https://doi.org/10.1093/hmg/ddy114},
  abstract = {Precision medicine can utilize new techniques in order to more effectively translate research findings into clinical practice. In this article, we first explore the limitations of traditional study designs, which stem from (to name a few): massive cost for the assembly of large patient cohorts; non-representative patient data; and the astounding complexity of human biology. Second, we propose that harnessing electronic health records and mobile device biometrics coupled to longitudinal data may prove to be a solution to many of these problems by capturing a ‘real world’ phenotype. We envision that future biomedical research utilizing more precise approaches to patient care will utilize continuous and longitudinal data sources.},
}

@Article{Kourou2014,
  author    = {Kourou, Konstantina and Exarchos, Themis P. and Exarchos, Konstantinos P. and Karamouzis, Michalis V. and Fotiadis, Dimitrios I.},
  title     = {Machine learning applications in cancer prognosis and prediction},
  journal   = {Computational and structural biotechnology journal},
  year      = {2014},
  language  = {eng},
  volume    = {13},
  month     = nov,
  pages     = {8--17},
  issn      = {2001-0370},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/25750696},
  abstract  = {Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diagnosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent clinical management of patients. The importance of classifying cancer patients into high or low risk groups has led many research teams, from the biomedical and the bioinformatics field, to study the application of machine learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied in cancer research for the development of predictive models, resulting in effective and accurate decision making. Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an appropriate level of validation is needed in order for these methods to be considered in the everyday clinical practice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression. The predictive models discussed here are based on various supervised ML techniques as well as on different input features and data samples. Given the growing trend on the application of ML methods in cancer research, we present here the most recent publications that employ these techniques as an aim to model cancer risk or patient outcomes.},
  comment   = {25750696[pmid] PMC4348437[pmcid]},
  database  = {PubMed},
  publisher = {Research Network of Computational and Structural Biotechnology},
}

@Article{Mosley2018,
  author   = {Mosley, Jonathan D. and Feng, QiPing and Wells, Quinn S. and Van Driest, Sara L. and Shaffer, Christian M. and Edwards, Todd L. and Bastarache, Lisa and Wei, Wei-Qi and Davis, Lea K. and McCarty, Catherine A. and Thompson, Will and Chute, Christopher G. and Jarvik, Gail P. and Gordon, Adam S. and Palmer, Melody R. and Crosslin, David R. and Larson, Eric B. and Carrell, David S. and Kullo, Iftikhar J. and Pacheco, Jennifer A. and Peissig, Peggy L. and Brilliant, Murray H. and Linneman, James G. and Namjou, Bahram and Williams, Marc S. and Ritchie, Marylyn D. and Borthwick, Kenneth M. and Verma, Shefali S. and Karnes, Jason H. and Weiss, Scott T. and Wang, Thomas J. and Stein, C. Michael and Denny, Josh C. and Roden, Dan M.},
  title    = {A study paradigm integrating prospective epidemiologic cohorts and electronic health records to identify disease biomarkers},
  journal  = {Nature Communications},
  year     = {2018},
  volume   = {9},
  number   = {1},
  month    = aug,
  pages    = {3522},
  issn     = {2041-1723},
  url      = {https://doi.org/10.1038/s41467-018-05624-4},
  abstract = {Defining the full spectrum of human disease associated with a biomarker is necessary to advance the biomarker into clinical practice. We hypothesize that associating biomarker measurements with electronic health record (EHR) populations based on shared genetic architectures would establish the clinical epidemiology of the biomarker. We use Bayesian sparse linear mixed modeling to calculate SNP weightings for 53 biomarkers from the Atherosclerosis Risk in Communities study. We use the SNP weightings to computed predicted biomarker values in an EHR population and test associations with 1139 diagnoses. Here we report 116 associations meeting a Bonferroni level of significance. A false discovery rate (FDR)-based significance threshold reveals more known and undescribed associations across a broad range of biomarkers, including biometric measures, plasma proteins and metabolites, functional assays, and behaviors. We confirm an inverse association between LDL-cholesterol level and septicemia risk in an independent epidemiological cohort. This approach efficiently discovers biomarker-disease associations.},
  refid    = {Mosley2018},
}

@Article{Menachemi2011,
  author    = {Menachemi, Nir and Collum, Taleah H.},
  title     = {Benefits and drawbacks of electronic health record systems},
  journal   = {Risk management and healthcare policy},
  year      = {2011},
  language  = {eng},
  volume    = {4},
  month     = may,
  pages     = {47--55},
  issn      = {1179-1594},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/22312227},
  abstract  = {The Health Information Technology for Economic and Clinical Health (HITECH) Act of 2009 that was signed into law as part of the "stimulus package" represents the largest US initiative to date that is designed to encourage widespread use of electronic health records (EHRs). In light of the changes anticipated from this policy initiative, the purpose of this paper is to review and summarize the literature on the benefits and drawbacks of EHR systems. Much of the literature has focused on key EHR functionalities, including clinical decision support systems, computerized order entry systems, and health information exchange. Our paper describes the potential benefits of EHRs that include clinical outcomes (eg, improved quality, reduced medical errors), organizational outcomes (eg, financial and operational benefits), and societal outcomes (eg, improved ability to conduct research, improved population health, reduced costs). Despite these benefits, studies in the literature highlight drawbacks associated with EHRs, which include the high upfront acquisition costs, ongoing maintenance costs, and disruptions to workflows that contribute to temporary losses in productivity that are the result of learning a new system. Moreover, EHRs are associated with potential perceived privacy concerns among patients, which are further addressed legislatively in the HITECH Act. Overall, experts and policymakers believe that significant benefits to patients and society can be realized when EHRs are widely adopted and used in a "meaningful" way.},
  comment   = {22312227[pmid] PMC3270933[pmcid]},
  database  = {PubMed},
  publisher = {Dove Medical Press},
}

@Article{Carroll2015,
  author   = {Carroll, Robert J. and Eyler, Anne E. and Denny, Joshua C.},
  title    = {Intelligent use and clinical benefits of electronic health records in rheumatoid arthritis},
  journal  = {Expert review of clinical immunology},
  year     = {2015},
  language = {eng},
  volume   = {11},
  number   = {3},
  month    = mar,
  pages    = {329--337},
  issn     = {1744-666X},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/25660652},
  abstract = {In the past 10 years, electronic health records (EHRs) have had growing impact in clinical care. EHRs efficiently capture and reuse clinical information, which can directly benefit patient care by guiding treatments and providing effective reminders for best practices. The increased adoption has also lead to more complex implementations, including robust, disease-specific tools, such as for rheumatoid arthritis (RA). In addition, the data collected through normal clinical care is also used in secondary research, helping to refine patient treatment for the future. Although few studies have directly demonstrated benefits for direct clinical care of RA, the opposite is true for EHR-based research - RA has been a particularly fertile ground for clinical and genomic research that have leveraged typically advanced informatics methods to accurately define RA populations. We discuss the clinical impact of EHRs in RA treatment and their impact on secondary research, and provide recommendations for improved utility in future EHR installations.},
  comment  = {25660652[pmid] PMC4518025[pmcid]},
  database = {PubMed},
  edition  = {2015/02/08},
}

@Article{Ross2014,
  author    = {Ross, M. K. and Wei, W. and Ohno-Machado, L.},
  title     = {"Big data" and the electronic health record},
  journal   = {Yearbook of medical informatics},
  year      = {2014},
  language  = {eng},
  volume    = {9},
  number    = {1},
  month     = aug,
  pages     = {97--104},
  issn      = {0943-4747},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/25123728},
  abstract  = {OBJECTIVES: Implementation of Electronic Health Record (EHR) systems continues to expand. The massive number of patient encounters results in high amounts of stored data. Transforming clinical data into knowledge to improve patient care has been the goal of biomedical informatics professionals for many decades, and this work is now increasingly recognized outside our field. In reviewing the literature for the past three years, we focus on "big data" in the context of EHR systems and we report on some examples of how secondary use of data has been put into practice. METHODS: We searched PubMed database for articles from January 1, 2011 to November 1, 2013. We initiated the search with keywords related to "big data" and EHR. We identified relevant articles and additional keywords from the retrieved articles were added. Based on the new keywords, more articles were retrieved and we manually narrowed down the set utilizing predefined inclusion and exclusion criteria. RESULTS: Our final review includes articles categorized into the themes of data mining (pharmacovigilance, phenotyping, natural language processing), data application and integration (clinical decision support, personal monitoring, social media), and privacy and security. CONCLUSION: The increasing adoption of EHR systems worldwide makes it possible to capture large amounts of clinical data. There is an increasing number of articles addressing the theme of "big data", and the concepts associated with these articles vary. The next step is to transform healthcare big data into actionable knowledge.},
  comment   = {25123728[pmid] PMC4287068[pmcid]},
  database  = {PubMed},
  publisher = {Schattauer GmbH},
}

@Article{Kruse2018,
  author    = {Kruse, Clemens Scott and Stein, Anna and Thomas, Heather and Kaur, Harmander},
  title     = {The use of Electronic Health Records to Support Population Health: A Systematic Review of the Literature},
  journal   = {Journal of medical systems},
  year      = {2018},
  language  = {eng},
  volume    = {42},
  number    = {11},
  month     = sep,
  pages     = {214--214},
  issn      = {0148-5598},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/30269237},
  abstract  = {Electronic health records (EHRs) have emerged among health information technology as "meaningful use" to improve the quality and efficiency of healthcare, and health disparities in population health. In other instances, they have also shown lack of interoperability, functionality and many medical errors. With proper implementation and training, are electronic health records a viable source in managing population health? The primary objective of this systematic review is to assess the relationship of electronic health records' use on population health through the identification and analysis of facilitators and barriers to its adoption for this purpose. Authors searched Cumulative Index of Nursing and Allied Health Literature (CINAHL) and MEDLINE (PubMed), 10/02/2012-10/02/2017, core clinical/academic journals, MEDLINE full text, English only, human species and evaluated the articles that were germane to our research objective. Each article was analyzed by multiple reviewers. Group members recognized common facilitators and barriers associated with EHRs effect on population health. A final list of articles was selected by the group after three consensus meetings (n = 55). Among a total of 26 factors identified, 63% (147/232) of those were facilitators and 37% (85/232) barriers. About 70% of the facilitators consisted of productivity/efficiency in EHRs occurring 33 times, increased quality and data management each occurring 19 times, surveillance occurring 17 times, and preventative care occurring 15 times. About 70% of the barriers consisted of missing data occurring 24 times, no standards (interoperability) occurring 13 times, productivity loss occurring 12 times, and technology too complex occurring 10 times. The analysis identified more facilitators than barriers to the use of the EHR to support public health. Wider adoption of the EHR and more comprehensive standards for interoperability will only enhance the ability for the EHR to support this important area of surveillance and disease prevention. This review identifies more facilitators than barriers to using the EHR to support public health, which implies a certain level of usability and acceptance to use the EHR in this manner. The public-health industry should combine their efforts with the interoperability projects to make the EHR both fully adopted and fully interoperable. This will greatly increase the availability, accuracy, and comprehensiveness of data across the country, which will enhance benchmarking and disease surveillance/prevention capabilities.},
  comment   = {30269237[pmid] PMC6182727[pmcid]},
  database  = {PubMed},
  publisher = {Springer US},
}

@Article{Gooding2019,
  author   = {Gooding, Christopher R. and Cundall-Curry, Duncan and Lawrence, John E. and Stewart, Max E. and Fountain, Daniel M.},
  title    = {The use of an electronic health record system reduces errors in the National Hip Fracture Database},
  journal  = {ageing},
  year     = {2019},
  volume   = {48},
  number   = {2},
  month    = mar,
  pages    = {285--290},
  issn     = {0002-0729},
  url      = {https://doi.org/10.1093/ageing/afy177},
  abstract = {to compare the validity of data submitted from a UK level 1 trauma centre to the National Hip Fracture Database (NHFD) before and after the introduction of an electronic health record system (EHRS).a total of 3224 records were reviewed from July 2009 to July 2017. 2,133 were submitted between July 2009 and October 2014 and 1,091 between October 2014 and July 2017, representing data submitted before and after the introduction of the EHRS, respectively. Data submitted to the NHFD were scrutinised against locally held data.use of an EHRS was associated with significant reductions in NHFD errors. The operation coding error rate fell significantly from 23.2% (494/2133) to 7.6% (83/1091); P &lt; 0.001. Prior to EHRS introduction, of the 109 deaths recorded in the NHFD, 64 (59%) were incorrect. In the EHRS dataset, all the 112 recorded deaths were correct (P &lt; 0.001). There was no significant difference in the error rate for fracture coding. In the EHRS dataset, after controlling for sample month, entries utilising an operation note template with mandatory fields relevant to NHFD data were more likely to be error free than those not using the template (OR 2.69; 95% CI 1.92-3.78).this study highlights a potential benefit of EHR systems, which offer automated data collection for auditing purposes. However, errors in data submitted to the NHFD remain, particularly in cases where an NHFD-specific operation note template is not used. Clinician engagement with new technologies is vital to avoid human error and ensure database integrity.},
}

@Article{Huang2009,
  author    = {Huang, Desheng and Quan, Yu and He, Miao and Zhou, Baosen},
  title     = {Comparison of linear discriminant analysis methods for the classification of cancer based on gene expression data},
  journal   = {Journal of experimental \& clinical cancer research : CR},
  year      = {2009},
  language  = {eng},
  volume    = {28},
  number    = {1},
  month     = dec,
  pages     = {149--149},
  issn      = {0392-9078},
  url       = {https://www.ncbi.nlm.nih.gov/pubmed/20003274},
  abstract  = {BACKGROUND: More studies based on gene expression data have been reported in great detail, however, one major challenge for the methodologists is the choice of classification methods. The main purpose of this research was to compare the performance of linear discriminant analysis (LDA) and its modification methods for the classification of cancer based on gene expression data. METHODS: The classification performance of linear discriminant analysis (LDA) and its modification methods was evaluated by applying these methods to six public cancer gene expression datasets. These methods included linear discriminant analysis (LDA), prediction analysis for microarrays (PAM), shrinkage centroid regularized discriminant analysis (SCRDA), shrinkage linear discriminant analysis (SLDA) and shrinkage diagonal discriminant analysis (SDDA). The procedures were performed by software R 2.80. RESULTS: PAM picked out fewer feature genes than other methods from most datasets except from Brain dataset. For the two methods of shrinkage discriminant analysis, SLDA selected more genes than SDDA from most datasets except from 2-class lung cancer dataset. When comparing SLDA with SCRDA, SLDA selected more genes than SCRDA from 2-class lung cancer, SRBCT and Brain dataset, the result was opposite for the rest datasets. The average test error of LDA modification methods was lower than LDA method. CONCLUSIONS: The classification performance of LDA modification methods was superior to that of traditional LDA with respect to the average error and there was no significant difference between theses modification methods.},
  comment   = {20003274[pmid] PMC2800110[pmcid]},
  database  = {PubMed},
  publisher = {BioMed Central},
}

@Article{Ng2016,
  author   = {Ng, Kenney and Steinhubl, Steven R. and deFilippi, Christopher and Dey, Sanjoy and Stewart, Walter F.},
  title    = {Early Detection of Heart Failure Using Electronic Health Records: Practical Implications for Time Before Diagnosis, Data Diversity, Data Quantity, and Data Density},
  journal  = {Circulation. Cardiovascular quality and outcomes},
  year     = {2016},
  language = {eng},
  volume   = {9},
  number   = {6},
  month    = nov,
  pages    = {649--658},
  issn     = {1941-7713},
  url      = {https://www.ncbi.nlm.nih.gov/pubmed/28263940},
  abstract = {BACKGROUND: Using electronic health records data to predict events and onset of diseases is increasingly common. Relatively little is known, although, about the tradeoffs between data requirements and model utility. METHODS AND RESULTS: We examined the performance of machine learning models trained to detect prediagnostic heart failure in primary care patients using longitudinal electronic health records data. Model performance was assessed in relation to data requirements defined by the prediction window length (time before clinical diagnosis), the observation window length (duration of observation before prediction window), the number of different data domains (data diversity), the number of patient records in the training data set (data quantity), and the density of patient encounters (data density). A total of 1684 incident heart failure cases and 13 525 sex, age-category, and clinic matched controls were used for modeling. Model performance improved as (1) the prediction window length decreases, especially when <2 years; (2) the observation window length increases but then levels off after 2 years; (3) the training data set size increases but then levels off after 4000 patients; (4) more diverse data types are used, but, in order, the combination of diagnosis, medication order, and hospitalization data was most important; and (5) data were confined to patients who had ≥10 phone or face-to-face encounters in 2 years. CONCLUSIONS: These empirical findings suggest possible guidelines for the minimum amount and type of data needed to train effective disease onset predictive models using longitudinal electronic health records data.},
  comment  = {28263940[pmid] PMC5341145[pmcid]},
  database = {PubMed},
  edition  = {2016/11/08},
}

@InProceedings{Bian16-1,
  author    = {Bian, J. and Barnes, L. E. and Chen, G. and Xiong, H.},
  title     = {Early detection of diseases using electronic health records data and covariance-regularized linear discriminant analysis},
  booktitle = {2017 IEEE EMBS International Conference on Biomedical \& Health Informatics (BHI)},
  year      = {16-1},
  pages     = {457--460},
  journal   = {2017 IEEE EMBS International Conference on Biomedical \& Health Informatics (BHI)},
  keywords  = {data mining, diseases, electronic health records, health care, medical computing, medical disorders, patient diagnosis, statistical analysis, electronic health record data, covariance-regularized linear discriminant analysis, health care settings, data mining tools, EHR-based disease early detection, statistical prediction methods, covariance-regularized LDA classifiers, diagnosis-frequency vector data representation, sparse precision matrix estimator, algorithm analysis, graphical lasso estimator, LDA model misclassification rate, mental health disorders, Diseases, Training, Covariance matrices, Support vector machines, Testing, Sensitivity, Algorithm design and analysis},
}

@Book{Murata2015,
  author        = {Murata, Chiharu and Belén Ramírez, Ana and Ramírez, Guadalupe and Cruz, Alonso and Morales, José and Lugo-Reyes, Saúl},
  title         = {Discriminant analysis to predict the clinical diagnosis of primary immunodeficiencies: A preliminary report},
  year          = {2015},
  volume        = {62},
  pages         = {125--33},
  __markedentry = {[shane:]},
  journal       = {Revista alergia Mexico (Tecamachalco, Puebla, Mexico : 1993)},
  month         = apr,
}

@Article{Kenney2016,
  author        = {Kenney, Ng and Steinhubl Steven, R. and deFilippi Christopher and Sanjoy, Dey and Stewart Walter, F.},
  title         = {Early Detection of Heart Failure Using Electronic Health Records},
  journal       = {Circulation: Cardiovascular Quality and Outcomes},
  year          = {2016},
  volume        = {9},
  number        = {6},
  month         = nov,
  pages         = {649--658},
  doi           = {10.1161/circoutcomes.116.002797},
  url           = {https://doi.org/10.1161/CIRCOUTCOMES.116.002797},
  __markedentry = {[shane:6]},
  comment       = {doi: 10.1161/CIRCOUTCOMES.116.002797},
  publisher     = {American Heart Association},
}

@Comment{jabref-meta: databaseType:biblatex;}
